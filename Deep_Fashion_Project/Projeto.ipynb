{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs  # Needs to be imported because of chinese characters\n",
    "import pandas as pd\n",
    "from PIL import *\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import sys\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "# GLOBAL\n",
    "\n",
    "#root = os.getcwd()\n",
    "root = '/home/joao/Projeto/Deep_Fashion_Project/'\n",
    "dataset_folder_path = os.path.join(root, 'Dataset')\n",
    "\n",
    "Anno_path = os.path.join(dataset_folder_path, 'Anno')\n",
    "list_attr_cloth = os.path.join(Anno_path, 'list_attr_cloth.txt')\n",
    "list_attr_items = os.path.join(Anno_path, 'list_attr_items.txt')\n",
    "list_attr_type = os.path.join(Anno_path, 'list_attr_type.txt')\n",
    "list_bbox_consumer2shop = os.path.join(Anno_path, 'list_bbox_consumer2shop.txt')\n",
    "list_item_consumer2shop = os.path.join(Anno_path, 'list_item_consumer2shop.txt')\n",
    "list_landmarks_consumer2shop = os.path.join(Anno_path, 'list_landmarks_consumer2shop.txt')\n",
    "\n",
    "Eval_path = os.path.join(dataset_folder_path, 'Eval')\n",
    "list_eval_partition = os.path.join(Eval_path, 'list_eval_partition.txt')\n",
    "\n",
    "Img_path = os.path.join(dataset_folder_path, 'Img')\n",
    "data_order = 'tf'\n",
    "#img_dtype = tables.UInt8Atom()  # dtype in which the images will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef save_hdf5_file(file, path):\\n    return 1\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#from config import *\n",
    "\n",
    "# Return list of lists [category_id, 'category_name_string', category_type_id]\n",
    "\n",
    "def get_category_id_name_type(path_list_attr_cloth):\n",
    "    category_list = []\n",
    "    with codecs.open(path_list_attr_cloth, 'r', 'utf-8') as file_list_attr_clothes:\n",
    "        next(file_list_attr_clothes)\n",
    "        next(file_list_attr_clothes)\n",
    "        for idx, line in enumerate(file_list_attr_clothes, 1):\n",
    "            category_name = line[24:66].strip().replace(' ', '_').upper()\n",
    "            category_attribute_type = int(line[-5:].strip())\n",
    "            category_list.append([idx, category_name, category_attribute_type])\n",
    "    return category_list\n",
    "\n",
    "\n",
    "# Return dictionary attr_type_dict = {'category_type_id': 'category_type_name'}\n",
    "def generate_attr_type_dict(path_list_attr_type):\n",
    "    attr_type_dict = dict()\n",
    "    with codecs.open(path_list_attr_type, 'r', 'utf-8') as file_list_attr_clothes:\n",
    "        next(file_list_attr_clothes)\n",
    "        next(file_list_attr_clothes)\n",
    "        for idx, line in enumerate(file_list_attr_clothes, 1):\n",
    "            attr_type_name = line[-37:].strip().replace(' ', '_').upper()\n",
    "            attr_type_dict[idx] = attr_type_name\n",
    "    return attr_type_dict\n",
    "\n",
    "\n",
    "# merge into list of lists [category_id, 'category_name_string', category_type_id, 'category_type_id']\n",
    "def merge_attr_types_names(attr_type_dict, category_list):\n",
    "    for category_id in category_list:\n",
    "        category_id.append(attr_type_dict[category_id[-1]])\n",
    "    return category_list\n",
    "\n",
    "\n",
    "# build three sets with unique item ids according to train/test/eval partition\n",
    "def get_item_ids_partition_sets(path_list_eval_partition):\n",
    "    train_ids = set()\n",
    "    val_ids = set()\n",
    "    test_ids = set()\n",
    "    with codecs.open(path_list_eval_partition, 'r', 'utf-8') as file_list_eval_partition:\n",
    "        next(file_list_eval_partition)\n",
    "        next(file_list_eval_partition)\n",
    "        for line in file_list_eval_partition:\n",
    "            if line.split()[3] == 'train':\n",
    "                train_ids.add(line.split()[2])\n",
    "            elif line.split()[3] == 'val':\n",
    "                val_ids.add(line.split()[2])\n",
    "            else:\n",
    "                test_ids.add(line.split()[2])\n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "#creates databse with all path of images and its partition group\n",
    "def gen_processed_list_eval_partition(path_list_eval_partition):\n",
    "    df_list_eval_partition = pd.read_table(path_list_eval_partition,\n",
    "                                           delim_whitespace=True, skiprows=0, header=1)\n",
    "    consumer_files = df_list_eval_partition.drop('image_pair_name_2', axis=1).drop_duplicates()\n",
    "    consumer_files = consumer_files.rename(columns={'image_pair_name_1': 'image_name'})\n",
    "    shop_files = df_list_eval_partition.drop('image_pair_name_1', axis=1).drop_duplicates()\n",
    "    shop_files = shop_files.rename(columns={'image_pair_name_2': 'image_name'})\n",
    "    processed_list_eval_partition = consumer_files.append(shop_files, ignore_index=True)\n",
    "    return processed_list_eval_partition\n",
    "\n",
    "#Generate database with all annotatios\n",
    "def gen_full_anno(path_list_eval_partition, path_list_landmarks_consumer2shop,\n",
    "                  path_list_bbox_consumer2shop, path_list_attr_items,\n",
    "                  bbox=True, item_features = True):\n",
    "    processed_list_eval_partition = gen_processed_list_eval_partition(path_list_eval_partition)\n",
    "    landmarks_consumer2shop = pd.read_table(path_list_landmarks_consumer2shop,\n",
    "                                            delim_whitespace=True, skiprows=0, header=1)\n",
    "    full_anno = processed_list_eval_partition.merge(landmarks_consumer2shop,\n",
    "                                                    how='inner', on='image_name')\n",
    "    if bbox:\n",
    "        bbox_consumer2shop = pd.read_table(path_list_bbox_consumer2shop,\n",
    "                                                delim_whitespace=True, skiprows=0, header=1)\n",
    "        full_anno = full_anno.merge(bbox_consumer2shop, how='inner', on='image_name')\n",
    "    if item_features:\n",
    "        col = ['item_id'] + ['Attr' + str(i) for i in range(1, 304)]\n",
    "        attr_consumer2shop = pd.read_table(path_list_attr_items,\n",
    "                                                delim_whitespace=True, skiprows=2, header=None, names=col)\n",
    "        full_anno = full_anno.merge(attr_consumer2shop,how='outer', on='item_id', validate=\"m:1\")\n",
    "    return full_anno\n",
    "#Select folder of images\n",
    "def gets_from_scope(full_anno,CLOTHING = True, DRESSES=True, TOPS=True, TROUSERS=True ):\n",
    "    full_anno['folders'] = full_anno.image_name.str.split('/').str[1]\n",
    "    if not CLOTHING:\n",
    "        full_anno = full_anno[full_anno.folders != 'CLOTHING']\n",
    "    if not DRESSES:\n",
    "        full_anno = full_anno[full_anno.folders != 'DRESSES']\n",
    "    if not TOPS:\n",
    "        full_anno = full_anno[full_anno.folders != 'TOPS']\n",
    "    if not TROUSERS:\n",
    "        full_anno = full_anno[full_anno.folders != 'TROUSERS']\n",
    "    return full_anno\n",
    "#split database in train, eval and test\n",
    "def split_full_anno(df_full_anno):\n",
    "    train = df_full_anno[df_full_anno.evaluation_status == 'train']\n",
    "    eval = df_full_anno[df_full_anno.evaluation_status == 'val']\n",
    "    test = df_full_anno[df_full_anno.evaluation_status == 'test']\n",
    "    return train, eval, test\n",
    "\n",
    "\n",
    "#gera lista de caminho para leitura das imagens\n",
    "def generate_path_list(data):\n",
    "    return data.image_name.tolist()\n",
    "\n",
    "\n",
    "# Encontra tamanho mÃ¡ximo das fotos\n",
    "def get_maximal_sizes(data_train):\n",
    "    i_max = 0\n",
    "    j_max = 0\n",
    "    for img in data_train:\n",
    "        if img.shape[0] > i_max:\n",
    "            i_max = img.shape[0]\n",
    "        if img.shape[1] > j_max:\n",
    "            j_max = img.shape[1]\n",
    "    return(i_max,j_max)\n",
    "\n",
    "\n",
    "# gera a lista de np.arrays referentes a imagem\n",
    "def generate_list_images(image_path_list):\n",
    "    return [(cv2.imread(os.path.join(Img_path,fname))) for fname in train]\n",
    "\n",
    "\n",
    "# Adiciona faixas pretas abaixo e a esquerda das figuras\n",
    "def create_numpy_data(image_path_list, Img_path, i_max = 301, j_max = 301):\n",
    "        \n",
    "    img = cv2.imread(os.path.join(Img_path,image_path_list[0]))\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    i_img, j_img = img.shape[0], img.shape[1]\n",
    "    img = np.hstack((img,np.zeros((i_img, j_max-j_img,3))))\n",
    "    img = np.vstack((img, np.zeros((i_max-i_img, j_max, 3))))\n",
    "    data = img\n",
    "    for fname in image_path_list[1:]:\n",
    "        img = cv2.imread(os.path.join(Img_path,fname))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        i_img, j_img = img.shape[0], img.shape[1]\n",
    "        img = np.hstack((img,np.zeros((i_img, j_max-j_img,3))))\n",
    "        img = np.vstack((img, np.zeros((i_max-i_img, j_max, 3))))\n",
    "        data = np.vstack((data,img))\n",
    "    return data\n",
    "'''\n",
    "def save_hdf5_file(file, path):\n",
    "    with h5py.File('name-of-file.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"name-of-dataset\",  data=file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #category_list = get_category_id_name_type(list_attr_cloth)\n",
    "    \n",
    "    #attr_type_dict = generate_attr_type_dict(list_attr_type)\n",
    "    \n",
    "    #category_data = merge_attr_types_names(attr_type_dict, category_list)\n",
    "    \n",
    "    #train_ids_set, val_ids_set, test_ids_set = get_item_ids_partition_sets(list_eval_partition)\n",
    "    \n",
    "    df_full_anno = gen_full_anno(list_eval_partition, list_landmarks_consumer2shop,\n",
    "                  list_bbox_consumer2shop, list_attr_items, item_features= True)\n",
    "    \n",
    "    clothing_full = gets_from_scope(df_full_anno, True, False, False, False)\n",
    "    \n",
    "    df_train, df_val, df_test = split_full_anno(clothing_full)\n",
    "    \n",
    "    #clothing_train_list_path = generate_path_list(df_train)\n",
    "    \n",
    "    #clothing_val_list_path = generate_path_list(df_val)\n",
    "    \n",
    "    #clothing_test_list_path = generate_path_list(df_test)\n",
    "    \n",
    "    #data_clothing_train = create_numpy_data(clothing_train_list_path ,i_max = 301,\\\n",
    "                                            #j_max = 301, Img_path = Img_path)\n",
    "    \n",
    "    #data_clothing_val = create_numpy_data(clothing_val_list_path ,i_max = 301,\\\n",
    "    #                                      j_max = 301, Img_path = Img_path)\n",
    "    \n",
    "    #data_clothing_test = create_numpy_data(clothing_test_list_path , i_max = 301,\\\n",
    "     #                                      j_max = 301, Img_path = Img_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For upper-body clothing does not have landmark 7 and 8\n",
    "#these columns will be droped\n",
    "\n",
    "clothing_train_y_landmark = df_train.loc[:, 'clothes_type_x':'landmark_location_y_6'].as_matrix()\\\n",
    ".astype(dtype = np.int16)\n",
    "\n",
    "clothing_test_y_landmark = df_test.loc[:, 'clothes_type_x':'landmark_location_y_6'].as_matrix()\\\n",
    ".astype(dtype = np.int16)\n",
    "\n",
    "clothing_val_y_landmark = df_val.loc[:, 'clothes_type_x':'landmark_location_y_6'].as_matrix()\\\n",
    ".astype(dtype = np.int16)\n",
    "\n",
    "\n",
    "clothing_train_y_bbox = df_train.loc[:,'clothes_type_y':'y_2'].as_matrix()\\\n",
    ".astype(dtype = np.int16)\n",
    "\n",
    "\n",
    "clothing_test_y_bbox = df_test.loc[:,'clothes_type_y':'y_2'].as_matrix()\\\n",
    ".astype(dtype = np.int16)\n",
    "\n",
    "\n",
    "clothing_val_y_bbox = df_val.loc[:,'clothes_type_y':'y_2'].as_matrix()\\\n",
    ".astype(dtype = np.int16)\n",
    "\n",
    "clothing_train_y_bbox = df_train.loc[:,'clothes_type_y':'y_2'].as_matrix()\\\n",
    ".astype(dtype = np.int16)\n",
    "\n",
    "clothing_train_y_features = df_train.loc[:,'Attr1':'Attr303'].as_matrix()\\\n",
    ".astype(dtype = np.int8) \n",
    "\n",
    "clothing_test_y_features = df_test.loc[:,'Attr1':'Attr303'].as_matrix()\\\n",
    ".astype(dtype = np.int8) \n",
    "\n",
    "clothing_val_y_features = df_val.loc[:,'Attr1':'Attr303'].as_matrix()\\\n",
    ".astype(dtype = np.int8) \n",
    "\n",
    "\n",
    "hf = h5py.File('clothing_data_y_train_np.h5', 'w')\n",
    "hf.create_dataset('landmark_train', data=clothing_train_y_landmark)\n",
    "hf.create_dataset('bbox_train', data=clothing_train_y_bbox)\n",
    "hf.create_dataset('features_train', data=clothing_train_y_features)\n",
    "hf.close()\n",
    "\n",
    "\n",
    "hf = h5py.File('clothing_data_y_test_np.h5', 'w')\n",
    "hf.create_dataset('landmark_test', data=clothing_test_y_landmark)\n",
    "hf.create_dataset('bbox_test', data=clothing_test_y_bbox)\n",
    "hf.create_dataset('features_test', data=clothing_test_y_features)\n",
    "hf.close()\n",
    "\n",
    "hf = h5py.File('clothing_data_y_val_np.h5', 'w')\n",
    "hf.create_dataset('landmark_val', data=clothing_val_y_landmark)\n",
    "hf.create_dataset('bbox_val', data=clothing_val_y_bbox)\n",
    "hf.create_dataset('features_val', data=clothing_val_y_features)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7628\n",
       "2    4949\n",
       "Name: clothes_type_y, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clothing_full.clothes_type_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011518001556396484"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df_train.image_name.tolist()\n",
    "#del(df_train, df_full_anno,df_eval,df_test\n",
    "\n",
    "t0 = time.time()\n",
    "data_train = [(cv2.imread(os.path.join(Img_path,fname))) for fname in train[:10]]\n",
    "#with open('data_train.pkl', 'wb') as f:\n",
    " #   pickle.dump(data_train, f)\n",
    "(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#df_train.iloc[:,].astype('int16').memory_usage(deep.apply(pd.to_numeric,downcast ='unsigned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
